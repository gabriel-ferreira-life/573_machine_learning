{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = '../data/car_evaluation.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions: (1381, 6), (1381,)\n",
      "Testing set dimensions: (346, 6), (346,)\n"
     ]
    }
   ],
   "source": [
    "data = read_data(data_loc)\n",
    "\n",
    "# Train data\n",
    "train_data = data['train']\n",
    "X_train = np.array([item[0] for item in train_data])\n",
    "y_train = np.array([item[1] for item in train_data])\n",
    "\n",
    "# Test data\n",
    "test_data = data['test']\n",
    "X_test = np.array([item[0] for item in test_data])\n",
    "y_test = np.array([item[1] for item in test_data])\n",
    "\n",
    "# Display data dimentions\n",
    "print(f'Training set dimensions: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Testing set dimensions: {X_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    \"\"\"Compute entropy of data.\n",
    "\n",
    "    Args:\n",
    "        data: A list of data points [(x_0, y_0), ..., (x_n, y_n)]\n",
    "\n",
    "    Returns:\n",
    "        entropy of data (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse data\n",
    "    y = np.array([item[1] for item in data])\n",
    "\n",
    "    # Lenth of the target\n",
    "    len_y = len(y)\n",
    "\n",
    "    # Frequency of each label\n",
    "    labels, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    # Compute entropy using label probabilities\n",
    "    entropy = 0\n",
    "    for count in counts:\n",
    "        probability = count / len_y\n",
    "        if probability > 0:\n",
    "            entropy -= probability * np.log2(probability)\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain(data, feature):\n",
    "    \"\"\"Compute the gain of data of splitting by feature.\n",
    "\n",
    "    Args:\n",
    "        data: A list of data points [(x_0, y_0), ..., (x_n, y_n)]\n",
    "        feature: index of feature to split the data\n",
    "\n",
    "    Returns:\n",
    "        gain of splitting data by feature\n",
    "    \"\"\"\n",
    "\n",
    "    # Base Entropy\n",
    "    base_entropy = entropy(data)\n",
    "\n",
    "    # Unique values of the given feature\n",
    "    values = set([x[feature] for x, y in data])\n",
    "\n",
    "    # Calculate feature weighted entropy\n",
    "    weighted_entropy = 0\n",
    "    for value in values:\n",
    "        subset = [(x, y) for x, y in data if x[feature] == value]\n",
    "        subset_entropy = entropy(subset)\n",
    "        subset_probability = len(subset)/ len(data)\n",
    "        weighted_entropy += subset_probability*subset_entropy\n",
    "    \n",
    "    # Return Information Gain of the feature\n",
    "    return base_entropy - weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature(data):\n",
    "    \"\"\"Find the best feature to split data.\n",
    "\n",
    "    Args:\n",
    "        data: A list of data points [(x_0, y_0), ..., (x_n, y_n)]\n",
    "\n",
    "    Returns:\n",
    "        index of feature to split data\n",
    "    \"\"\"\n",
    "    # Initialize features\n",
    "    best_feature = -1\n",
    "    max_gain = np.float64('-inf')\n",
    "\n",
    "    # Loop through the range of columns in the dataset\n",
    "    for col in range(len(data[0][0])):\n",
    "\n",
    "        # Compute the feature gain\n",
    "        feature_ig = gain(data,col)\n",
    "\n",
    "        # Compare current feature_ig with the max_gain\n",
    "        if feature_ig > max_gain:\n",
    "            max_gain = feature_ig\n",
    "            best_feature = col\n",
    "\n",
    "    # Return the feature with the highest IG\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data):\n",
    "    ys = {}\n",
    "    for x, y in data:\n",
    "        ys[y] = ys.get(y, 0) + 1\n",
    "    if len(ys) == 1:\n",
    "        return list(ys)[0]\n",
    "    feature = get_best_feature(data)\n",
    "    subtrees = {}\n",
    "\n",
    "    # Unique values of the selected feature\n",
    "    values = set([x[feature] for x, y in data])\n",
    "\n",
    "    # Split data and build subtrees recursively for each value of the feature\n",
    "    for value in values:\n",
    "        subset = [(x, y) for x, y in data if x[feature] == value]\n",
    "        subtrees[value] = build_tree(subset)\n",
    "\n",
    "    return Tree(feature, ys, subtrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from helper import read_data\n",
    "\n",
    "def test_tree_accuracy(data):\n",
    "    random.seed(1)\n",
    "    print('=' * 58)\n",
    "    trainlen = len(data['train'])\n",
    "\n",
    "    train_data = data['train']\n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    tree = build_tree(train_data[:int(trainlen * 0.8)])\n",
    "    print(\n",
    "        'Validate accuracy on tree without pruning ========>',\n",
    "        test_data(tree, train_data[int(trainlen * 0.8):]))\n",
    "    Ptree = prune_tree(\n",
    "        copy.deepcopy(tree), train_data[int(trainlen * 0.8):])\n",
    "    print(\n",
    "        'Validate accuracy on tree with pruning ===========>',\n",
    "        test_data(Ptree, train_data[int(trainlen * 0.8):]))\n",
    "    print(\n",
    "        'Test accuracy on tree without pruning ============>',\n",
    "        test_data(tree, data['test']))\n",
    "    print(\n",
    "        'Test accuracy on tree with pruning ===============>',\n",
    "        test_data(Ptree, data['test']))\n",
    "    print('Tree size without pruning ========================> %6d' % (\n",
    "        tree.size))\n",
    "    print('Tree size with pruning ===========================> %6d' % (\n",
    "        Ptree.size))\n",
    "    print('Tree depth without pruning =======================> %6d' % (\n",
    "        tree.depth))\n",
    "    print('Tree depth with pruning ==========================> %6d' % (\n",
    "        Ptree.depth))\n",
    "    print('=' * 58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "\n",
    "    def __init__(self, feature=None, ys={}, subtrees={}):\n",
    "        self.feature = feature\n",
    "        self.ys = ys\n",
    "        self.subtrees = subtrees\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        size = 1\n",
    "        for subtree in self.subtrees.values():\n",
    "            if type(subtree) == int:\n",
    "                size += 1\n",
    "            else:\n",
    "                size += subtree.size\n",
    "        return size\n",
    "\n",
    "    @property\n",
    "    def depth(self):\n",
    "        max_depth = 0\n",
    "        for subtree in self.subtrees.values():\n",
    "            if type(subtree) == int:\n",
    "                cur_depth = 1\n",
    "            else:\n",
    "                cur_depth = subtree.depth\n",
    "            max_depth = max(cur_depth, max_depth)\n",
    "        return max_depth + 1\n",
    "    \n",
    "def entropy(data):\n",
    "    \"\"\"Compute entropy of data.\n",
    "\n",
    "    Args:\n",
    "        data: A list of data points [(x_0, y_0), ..., (x_n, y_n)]\n",
    "\n",
    "    Returns:\n",
    "        entropy of data (float)\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse data\n",
    "    y = np.array([item[1] for item in data])\n",
    "\n",
    "    # Lenth of the target\n",
    "    len_y = len(y)\n",
    "\n",
    "    # Frequency of each label\n",
    "    labels, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    # Compute entropy using label probabilities\n",
    "    entropy = 0\n",
    "    for count in counts:\n",
    "        probability = count / len_y\n",
    "        if probability > 0:\n",
    "            entropy -= probability * np.log2(probability)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def gain(data, feature):\n",
    "    \"\"\"Compute the gain of data of splitting by feature.\n",
    "\n",
    "    Args:\n",
    "        data: A list of data points [(x_0, y_0), ..., (x_n, y_n)]\n",
    "        feature: index of feature to split the data\n",
    "\n",
    "    Returns:\n",
    "        gain of splitting data by feature\n",
    "    \"\"\"\n",
    "\n",
    "    # Base Entropy\n",
    "    base_entropy = entropy(data)\n",
    "\n",
    "    # Unique values of the given feature\n",
    "    values = set([x[feature] for x, y in data])\n",
    "\n",
    "    # Calculate feature weighted entropy\n",
    "    weighted_entropy = 0\n",
    "    for value in values:\n",
    "        subset = [(x, y) for x, y in data if x[feature] == value]\n",
    "        subset_entropy = entropy(subset)\n",
    "        subset_probability = len(subset)/ len(data)\n",
    "        weighted_entropy += subset_probability*subset_entropy\n",
    "    \n",
    "    # Return Information Gain of the feature\n",
    "    return base_entropy - weighted_entropy\n",
    "\n",
    "def get_best_feature(data):\n",
    "    \"\"\"Find the best feature to split data.\n",
    "\n",
    "    Args:\n",
    "        data: A list of data points [(x_0, y_0), ..., (x_n, y_n)]\n",
    "\n",
    "    Returns:\n",
    "        index of feature to split data\n",
    "    \"\"\"\n",
    "    # Initialize features\n",
    "    best_feature = -1\n",
    "    max_gain = np.float64('-inf')\n",
    "\n",
    "    # Loop through the range of columns in the dataset\n",
    "    for col in range(len(data[0][0])):\n",
    "\n",
    "        # Compute the feature gain\n",
    "        feature_ig = gain(data,col)\n",
    "\n",
    "        # Compare current feature_ig with the max_gain\n",
    "        if feature_ig > max_gain:\n",
    "            max_gain = feature_ig\n",
    "            best_feature = col\n",
    "\n",
    "    # Return the feature with the highest IG\n",
    "    return best_feature\n",
    "\n",
    "\n",
    "def build_tree(data):\n",
    "    ys = {}\n",
    "    for x, y in data:\n",
    "        ys[y] = ys.get(y, 0) + 1\n",
    "    if len(ys) == 1:\n",
    "        return list(ys)[0]\n",
    "    feature = get_best_feature(data)\n",
    "    subtrees = {}\n",
    "\n",
    "    # Unique values of the selected feature\n",
    "    values = set([x[feature] for x, y in data])\n",
    "\n",
    "    # Split data and build subtrees recursively for each value of the feature\n",
    "    for value in values:\n",
    "        subset = [(x, y) for x, y in data if x[feature] == value]\n",
    "        subtrees[value] = build_tree(subset)\n",
    "\n",
    "    return Tree(feature, ys, subtrees)\n",
    "\n",
    "\n",
    "def test_entry(tree, entry):\n",
    "    x, y = entry\n",
    "    if type(tree) == int:\n",
    "        return tree, y\n",
    "    if x[tree.feature] not in tree.subtrees:\n",
    "        return tree, max([(value, key) for key, value in tree.ys.items()])[1]\n",
    "    return test_entry(tree.subtrees[x[tree.feature]], entry)\n",
    "\n",
    "\n",
    "def test_data(tree, data):\n",
    "    count = 0\n",
    "    for d in data:\n",
    "        y_hat, y = test_entry(tree, d)\n",
    "        count += (y_hat == y)\n",
    "    return round(count / float(len(data)), 4)\n",
    "\n",
    "\n",
    "def prune_tree(tree, data):\n",
    "\n",
    "    # Check if the current tree node is a leaf node\n",
    "    if isinstance(tree, int):\n",
    "        return tree\n",
    "    \n",
    "    # Determine what to do if there's no data reaching the current node\n",
    "    if not data:\n",
    "        leaf_label = max(tree.ys, key=tree.ys.get) # Majority classs\n",
    "        return leaf_label\n",
    "\n",
    "    # Prune each subtree recursively\n",
    "    for value, subtree in tree.subtrees.items():\n",
    "        data_subtree = [(x, y) for x, y in data if x[tree.feature] == value]\n",
    "        tree.subtrees[value] = prune_tree(subtree, data_subtree)\n",
    "\n",
    "    # Attempt to collapse this node\n",
    "    leaf_label = max(tree.ys, key=tree.ys.get)\n",
    "    collapsed_accuracy = sum(1 for x, y in data if y == leaf_label) / len(data)\n",
    "    current_accuracy = test_data(tree, data)\n",
    "\n",
    "    # Only prune if collapsed accuracy improves by at least delta\n",
    "    if collapsed_accuracy > current_accuracy:\n",
    "        return leaf_label\n",
    "    else:\n",
    "        return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Validate accuracy on tree without pruning ========> 0.8881\n",
      "Validate accuracy on tree with pruning ===========> 0.917\n",
      "Test accuracy on tree without pruning ============> 0.8757\n",
      "Test accuracy on tree with pruning ===============> 0.8757\n",
      "Tree size without pruning ========================>    288\n",
      "Tree size with pruning ===========================>    198\n",
      "Tree depth without pruning =======================>      7\n",
      "Tree depth with pruning ==========================>      7\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "data = read_data(dataloc='../data/car_evaluation.csv')\n",
    "\n",
    "test_tree_accuracy(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
